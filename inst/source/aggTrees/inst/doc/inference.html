<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Inference</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>







<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Inference</h1>



<p>As described in the <a href="https://riccardo-df.github.io/aggTrees/articles/aggTrees-vignette.html">short
tutorial</a>, we can get standard errors for the GATEs by estimating via
OLS appropriate linear models. Then, under an “honesty” condition, we
can use the estimated standard errors to conduct valid inference about
the GATEs as usual, e.g., by constructing conventional confidence
intervals. In this article, we discuss which linear models are estimated
by the <code>inference_aggtree</code> function.</p>
<div id="honesty" class="section level2">
<h2>Honesty</h2>
<p>As mentioned above, we require an honesty condition to achieve valid
inference about the GATEs.</p>
<p>Honesty is a subsample-splitting technique that requires that
different observations are used to form the subgroups and estimate the
GATEs.</p>
<p>To this end, <code>inference_aggtree</code> always uses the honest
sample to estimate the linear models below, unless we called the
<code>build_aggtree</code> function without allocating any observation
to the honest sample (e.g., we set <code>honest_frac = 0</code> or we
used a vector of FALSEs for the <code>is_honest</code> argument).</p>
</div>
<div id="linear-models" class="section level2">
<h2>Linear Models</h2>
<p>When calling the <code>build_aggtree</code> function, the user can
control the GATE estimator employed by the routine by setting the
<code>method</code> argument. The <code>inference_aggtree</code>
function inherits this argument and selects the model to be estimated
accordingly.</p>
<div id="difference-in-mean-outcomes" class="section level4">
<h4>Difference in Mean Outcomes</h4>
<p>If <code>method</code> is set to <code>&quot;raw&quot;</code>, the
<code>inference_aggtree</code> function estimates via OLS the following
linear model:</p>
<p><span class="math display">\[\begin{equation}
    Y_i = \sum_{l = 1}^{|\mathcal{T_{\alpha}}|} L_{i, l} \, \gamma_l +
\sum_{l = 1}^{|\mathcal{T}_{\alpha}|} L_{i, l} \, D_i \, \beta_l +
\epsilon_i
\end{equation}\]</span></p>
<p>with <span class="math inline">\(|\mathcal{T}_{\alpha}|\)</span> the
number of leaves of a particular tree <span class="math inline">\(\mathcal{T}_{\alpha}\)</span>, and <span class="math inline">\(L_{i, l}\)</span> a dummy variable equal to one if
the <span class="math inline">\(i\)</span>-th unit falls in the <span class="math inline">\(l\)</span>-th leaf of <span class="math inline">\(\mathcal{T}_{\alpha}\)</span>.</p>
<p>Exploiting the random assignment to treatment, we can show that each
<span class="math inline">\(\beta_l\)</span> identifies the GATE in the
<span class="math inline">\(l\)</span>-th leaf.</p>
<p>Under honesty, the OLS estimator <span class="math inline">\(\hat{\beta}_l\)</span> of <span class="math inline">\(\beta_l\)</span> is root-<span class="math inline">\(n\)</span> consistent and asymptotically
normal.</p>
</div>
<div id="doubly-robust-scores" class="section level4">
<h4>Doubly-Robust Scores</h4>
<p>If <code>method</code> is set to <code>&quot;aipw&quot;</code>, the
<code>inference_aggtree</code> function estimates via OLS the following
linear model:</p>
<p><span class="math display">\[\begin{equation}
    \widehat{\Gamma}_i = \sum_{l = 1}^{|\mathcal{T}_{\alpha}|} L_{i, l}
\, \beta_l + \epsilon_i
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\Gamma_i\)</span> writes as:</p>
<p><span class="math display">\[\begin{equation*}
    \Gamma_i = \mu \left( 1, X_i \right) - \mu \left( 0, X_i \right) +
\frac{D_i \left[ Y_i - \mu \left( 1, X_i \right) \right]}{p \left( X_i
\right)}  - \frac{ \left( 1 - D_i \right) \left[ Y_i - \mu \left( 0, X_i
\right) \right]}{1 - p \left( X_i \right)}
\end{equation*}\]</span></p>
<p>with <span class="math inline">\(\mu \left(D_i, X_i \right) =
\mathbb{E} \left[ Y_i | D_i, Z_i \right]\)</span> the conditional mean
of <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(p \left( X_i \right) = \mathbb{P} \left( D_i = 1 |
X_i \right)\)</span> the propensity score.</p>
<p>The doubly-robust scores <span class="math inline">\(\Gamma_i\)</span> are inherited by the output of
the <code>build_aggtree</code> function.</p>
<p>As before, we can show that each <span class="math inline">\(\beta_l\)</span> identifies the GATE in the <span class="math inline">\(l\)</span>-th leaf, this time even in
observational studies.</p>
<p>Under honesty, the OLS estimator <span class="math inline">\(\hat{\beta}_l\)</span> of <span class="math inline">\(\beta_l\)</span> is root-<span class="math inline">\(n\)</span> consistent and asymptotically normal,
provided that the <span class="math inline">\(\Gamma_i\)</span> are
cross-fitted in the honest sample and that the product of the
convergence rates of the estimators of the nuisance functions <span class="math inline">\(\mu \left( \cdot, \cdot \right)\)</span> and <span class="math inline">\(p \left( \cdot \right)\)</span> is faster than
<span class="math inline">\(n^{1/2}\)</span>.</p>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
