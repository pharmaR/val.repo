% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fast_variants.R
\name{fast_adapdiscom}
\alias{fast_adapdiscom}
\title{Fast AdapDiscom}
\usage{
fast_adapdiscom(
  beta,
  x,
  y,
  x.tuning,
  y.tuning,
  x.test,
  y.test,
  nlambda,
  pp,
  robust = 0,
  n.l = 30,
  standardize = TRUE,
  itcp = TRUE,
  lambda.min.ratio = NULL,
  k.value = 1.5
)
}
\arguments{
\item{beta}{Vector, true beta coefficients (optional)}

\item{x}{Matrix, training data}

\item{y}{Vector, training response}

\item{x.tuning}{Matrix, tuning data}

\item{y.tuning}{Vector, tuning response}

\item{x.test}{Matrix, test data}

\item{y.test}{Vector, test response}

\item{nlambda}{Integer, number of lambda values}

\item{pp}{Vector, block sizes}

\item{robust}{Integer, 0 for classical, 1 for robust estimation}

\item{n.l}{Integer, number of tuning parameter (`l`) values for fast variants (number of alpha values)}

\item{standardize}{Logical, whether to standardize covariates. When TRUE, uses training data mean and standard deviation to standardize tuning and test sets. When robust=1, uses Huber-robust standard deviation estimates}

\item{itcp}{Logical, whether to include intercept}

\item{lambda.min.ratio}{Numeric, `lambda.min.ratio` sets the smallest lambda value in the grid, expressed as a fraction of `lambda.max`â€”the smallest lambda for which all coefficients are zero. By default, it is `0.0001` when the number of observations (`nobs`) exceeds the number of variables (`nvars`), and `0.01` when `nobs < nvars`. Using a very small value in the latter case can lead to overfitting.}

\item{k.value}{Numeric, tuning parameter for robust estimation}
}
\value{
List with estimation results
}
\description{
Fast AdapDiscom
}
\section{Value}{

The function returns a list containing the following components:

\describe{
  \item{err}{A multi-dimensional array storing the mean squared error (MSE) for all combinations of tuning parameters alpha and lambda.}
  \item{est.error}{The estimation error, calculated as the Euclidean distance between the estimated beta coefficients and the true beta (if provided).}
  \item{lambda}{The optimal lambda value chosen via cross-validation on the tuning set.}
  \item{alpha}{A vector of the optimal alpha values, also selected on the tuning set.}
  \item{train.error}{The mean squared error on the tuning set for the optimal parameter combination.}
  \item{test.error}{The mean squared error on the test set for the final, optimal model.}
  \item{y.pred}{The predicted values for the observations in the test set.}
  \item{R2}{The R-squared value, which measures the proportion of variance explained by the model on the test set.}
  \item{a0}{The intercept of the final model.}
  \item{a1}{The vector of estimated beta coefficients for the final model.}
  \item{select}{The number of non-zero coefficients, representing the number of selected variables.}
  \item{xtx}{The final regularized covariance matrix used to fit the optimal model.}
  \item{fpr}{The False Positive Rate (FPR) if the true beta is provided. It measures the proportion of irrelevant variables incorrectly selected.}
  \item{fnr}{The False Negative Rate (FNR) if the true beta is provided. It measures the proportion of relevant variables incorrectly excluded.}
  \item{lambda.all}{The complete vector of all lambda values tested during cross-validation.}
  \item{beta.cov.lambda.max}{The estimated beta coefficients using the maximum lambda value.}
  \item{time}{The total execution time of the function in seconds.}
}
}

\examples{
\donttest{
# Fast computation example with synthetic data
n <- 80
p <- 16

# Generate synthetic data with 2 blocks
set.seed(789)
x_train <- matrix(rnorm(n * p), n, p)
x_tuning <- matrix(rnorm(40 * p), 40, p)
x_test <- matrix(rnorm(25 * p), 25, p)

# True coefficients
beta_true <- c(rep(1.2, 3), rep(0, 5), rep(-0.8, 2), rep(0, 6))

# Response variables
y_train <- x_train \%*\% beta_true + rnorm(n, sd = 0.3)
y_tuning <- x_tuning \%*\% beta_true + rnorm(40, sd = 0.3)
y_test <- x_test \%*\% beta_true + rnorm(25, sd = 0.3)

# Block sizes (2 blocks of 8 variables each)  
pp <- c(8, 8)

# Run fast AdapDiscom (faster with fewer tuning parameters)
result <- fast_adapdiscom(beta = beta_true,
                          x = x_train, y = y_train,
                          x.tuning = x_tuning, y.tuning = y_tuning,
                          x.test = x_test, y.test = y_test,
                          nlambda = 15, pp = pp, n.l = 20)

# View results
print(paste("Test R-squared:", round(result$R2, 3)))
print(paste("Computation time:", round(result$time[3], 2), "seconds"))
}
}
